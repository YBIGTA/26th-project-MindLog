import torch
import clip
import torch.nn.functional as F
import requests
from io import BytesIO
from PIL import Image
from app.utils.places import places

class PlaceTagger:
    def __init__(self, model_name="ViT-B/32", threshold=0.3):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model, self.preprocess = clip.load(model_name, self.device)
        self.threshold = threshold
        self.labels = list(places.keys())

    def load_image(self, image_path: str):
        """ğŸ”¹ ë¡œì»¬ íŒŒì¼ ë° URL ì§€ì› (ì´ë¯¸ì§€ ë¡œë“œ)"""
        try:
            if image_path.startswith("http"):  # ğŸ”¹ URLì—ì„œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                response = requests.get(image_path, timeout=5)
                response.raise_for_status()
                image = Image.open(BytesIO(response.content)).convert("RGB")
            else:
                image = Image.open(image_path).convert("RGB")  # ğŸ”¹ ë¡œì»¬ íŒŒì¼ ë¡œë“œ
            return self.preprocess(image).unsqueeze(0).to(self.device)
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path}, ì˜¤ë¥˜: {e}")
            return None

    def predict_places(self, image_urls: list[str], top_k=3) -> dict:
        """
        ğŸ”¹ ì¥ì†Œ íƒœê¹… (ë°°ì¹˜ ì²˜ë¦¬)
        - ì—¬ëŸ¬ ì´ë¯¸ì§€ URLì„ ì…ë ¥ë°›ì•„ í•œ ë²ˆì— íƒœê¹… ìˆ˜í–‰
        - ì¥ì†Œ íƒœê¹… ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜
        """
        results = {}

        print(f"ğŸš€ ì¥ì†Œ íƒœê¹… ì‹œì‘: {len(image_urls)}ê°œì˜ ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘...")  # âœ… ì „ì²´ ì²˜ë¦¬ ì‹œì‘ ë¡œê·¸

        for image_url in image_urls:
            print(f"ğŸ” ì¥ì†Œ íƒœê¹… ì§„í–‰ ì¤‘: {image_url}")  # âœ… ê°œë³„ ì´ë¯¸ì§€ ì²˜ë¦¬ ë¡œê·¸
            image = self.load_image(image_url)
            if image is None:
                results[image_url] = {"error": "ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨"}
                print(f"âš ï¸ ì¥ì†Œ íƒœê¹… ì‹¤íŒ¨: {image_url} â†’ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨")  # âœ… ì‹¤íŒ¨ ë¡œê·¸ ì¶”ê°€
                continue

            try:
                text_inputs = clip.tokenize(self.labels).to(self.device)

                with torch.no_grad():
                    image_features = self.model.encode_image(image)
                    text_features = self.model.encode_text(text_inputs)
                    similarity = F.softmax(image_features @ text_features.T, dim=-1)

                    best_match_indices = similarity.argsort(descending=True)[0][:top_k]
                    best_places = [(self.labels[idx], float(similarity[0, idx].item())) for idx in best_match_indices]
                    valid_places = [place for place in best_places if place[1] >= self.threshold]

                    if valid_places:
                        results[image_url] = {
                            "place": places.get(valid_places[0][0], valid_places[0][0]),
                            "confidence": valid_places[0][1]
                        }
                        print(f"âœ… ì¥ì†Œ íƒœê¹… ì™„ë£Œ: {image_url} â†’ {results[image_url]}")  # âœ… ì„±ê³µ ë¡œê·¸ ì¶”ê°€
                    else:
                        results[image_url] = {"error": "No valid place detected"}
                        print(f"âš ï¸ ì¥ì†Œ íƒœê¹… ì‹¤íŒ¨: {image_url} â†’ No valid place detected")  # âœ… ì‹¤íŒ¨ ë¡œê·¸ ì¶”ê°€

            except Exception as e:
                results[image_url] = {"error": str(e)}
                print(f"âŒ ì¥ì†Œ íƒœê¹… ì˜¤ë¥˜: {image_url}, ì˜¤ë¥˜: {e}")  # âœ… ì˜ˆì™¸ ë°œìƒ ì‹œ ë¡œê·¸

        print("âœ… ì¥ì†Œ íƒœê¹… í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ")  # âœ… ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ë¡œê·¸
        return results
